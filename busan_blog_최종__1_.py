# -*- coding: utf-8 -*-
# busan_blog_NAVER.py
"""
ë¶€ì‚° ê³ ì‹œê³µê³  â†’ ë¸”ë¡œê·¸ ìë™í™” (ë„¤ì´ë²„ API ë²„ì „)

ì¹´ì¹´ì˜¤ â†’ ë„¤ì´ë²„ API ì™„ì „ ì „í™˜
âœ… ì‹¬ì‚¬ ë¶ˆí•„ìš”, ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
âœ… í•œêµ­ POI ë°ì´í„° ë” í’ë¶€
"""

import os
import re
import csv
import time
import math
import requests
import urllib.parse
from pathlib import Path
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

try:
    import pyperclip
except:
    pyperclip = None

# ====== CONFIG ======
BASE_URL = "https://www.busan.go.kr/news/gosiboard?articlNo=2"
START_PAGE = 1
END_PAGE = 3  # 1í˜ì´ì§€ â†’ 3í˜ì´ì§€ë¡œ í™•ëŒ€
KEYWORDS = ["ì¬ê°œë°œ", "ì¬ê±´ì¶•"]

# OSì— ë”°ë¼ ê²½ë¡œ ì„¤ì • (Windows/Linux ëª¨ë‘ ì§€ì›)
import platform
if platform.system() == "Windows":
    OUT_DIR = r"C:\Users\ì†¡ë¯¸ìŠ¹\downloaded_files"
    CSV_PATH = r"C:\Users\ì†¡ë¯¸ìŠ¹\download_manifest.csv"
    TESSERACT_EXE = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
else:
    # Linux (GitHub Actions)
    OUT_DIR = os.path.join(os.getcwd(), "downloaded_files")
    CSV_PATH = os.path.join(os.getcwd(), "download_manifest.csv")
    TESSERACT_EXE = "/usr/bin/tesseract"

# ë„¤ì´ë²„ API (ì¹´ì¹´ì˜¤ì—ì„œ ë³€ê²½)
NAVER_CLIENT_ID = "1i3u9jg46o"
NAVER_CLIENT_SECRET = "6FcXzVbgEM"

HEADLESS_LIST = True
HEADLESS_MAP = False
PAGE_SLEEP = 0.8
TIMEOUT = 15

# PopplerëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (PyMuPDF ì‚¬ìš©)
POPPLER_BIN = ""
OCR_MIN_CHARS = 300

# ====== ìœ í‹¸ ======
def ensure_dirs():
    for p in [OUT_DIR, os.path.join(OUT_DIR, "txt"), os.path.join(OUT_DIR, "blog_html"),
              os.path.join(OUT_DIR, "pdf_images"), os.path.join(OUT_DIR, "maps")]:
        Path(p).mkdir(parents=True, exist_ok=True)

def make_driver(headless=True):
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1400,1100")
    return webdriver.Chrome(options=opts)

def safe_text(el):
    try:
        return el.text.strip()
    except:
        return ""

def clean_filename(name):
    name = re.sub(r"\s*\(ìš©ëŸ‰[^)]*\)\s*$", "", name or "").strip()
    name = re.sub(r'[\\/:*?"<>|]', "_", name)
    return (name or "unnamed")[:180]

def normalize_text(text):
    t = (text or "").lower()
    t = re.sub(r"\s+", " ", t)
    return t.strip()

# ====== ë„¤ì´ë²„ API ======
def naver_geocode(address):
    """ë„¤ì´ë²„: ì£¼ì†Œ â†’ ì¢Œí‘œ"""
    try:
        # ë¶€ì‚°ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if "ë¶€ì‚°" not in address:
            address = f"ë¶€ì‚° {address}"
        
        url = "https://naveropenapi.apigw.ntruss.com/map-geocode/v2/geocode"
        headers = {
            "X-NCP-APIGW-API-KEY-ID": NAVER_CLIENT_ID,
            "X-NCP-APIGW-API-KEY": NAVER_CLIENT_SECRET
        }
        params = {"query": address}
        
        print(f"        ğŸ” ì£¼ì†Œ ê²€ìƒ‰: {address}")
        
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        
        if resp.status_code != 200:
            print(f"        âŒ API ì˜¤ë¥˜ (ìƒíƒœ: {resp.status_code})")
            return None
        
        data = resp.json()
        
        if data.get('addresses') and len(data['addresses']) > 0:
            addr = data['addresses'][0]
            lat = float(addr['y'])
            lng = float(addr['x'])
            print(f"        âœ… ì¢Œí‘œ: ({lat:.6f}, {lng:.6f})")
            return (lat, lng)
        
        # ì‹¤íŒ¨ ì‹œ ë²ˆì§€ìˆ˜ ì œê±°í•˜ê³  ì¬ì‹œë„
        if "ë²ˆì§€" in address:
            addr_without_benji = re.sub(r'\d+(?:-\d+)?ë²ˆì§€', '', address).strip()
            print(f"        ì¬ì‹œë„: {addr_without_benji}")
            
            params = {"query": addr_without_benji}
            resp = requests.get(url, headers=headers, params=params, timeout=10)
            data = resp.json()
            
            if data.get('addresses') and len(data['addresses']) > 0:
                addr = data['addresses'][0]
                lat = float(addr['y'])
                lng = float(addr['x'])
                print(f"        âœ… ì¢Œí‘œ (ì¬ì‹œë„): ({lat:.6f}, {lng:.6f})")
                return (lat, lng)
        
        print(f"        âŒ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None
        
    except Exception as e:
        print(f"        âš ï¸ Geocoding ì˜¤ë¥˜: {e}")
        return None

def naver_search_places(keyword, center_lat, center_lng, radius=2000):
    """ë„¤ì´ë²„: í‚¤ì›Œë“œë¡œ ì¥ì†Œ ê²€ìƒ‰"""
    try:
        # ë„¤ì´ë²„ Local Search API
        url = "https://naveropenapi.apigw.ntruss.com/map-place/v1/search"
        headers = {
            "X-NCP-APIGW-API-KEY-ID": NAVER_CLIENT_ID,
            "X-NCP-APIGW-API-KEY": NAVER_CLIENT_SECRET
        }
        
        # ê²€ìƒ‰ì–´ì— ë¶€ì‚° ì¶”ê°€
        query = f"ë¶€ì‚° {keyword}"
        
        params = {
            "query": query,
            "coordinate": f"{center_lng},{center_lat}",  # ë„¤ì´ë²„ëŠ” ê²½ë„,ìœ„ë„ ìˆœì„œ
            "display": 5  # ìµœëŒ€ 5ê°œ
        }
        
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        
        if resp.status_code != 200:
            print(f"        âš ï¸ API ì˜¤ë¥˜ ({keyword}): {resp.status_code}")
            return []
        
        data = resp.json()
        places = []
        
        if data.get('places'):
            for place in data['places']:
                try:
                    place_lat = float(place['y'])
                    place_lng = float(place['x'])
                    
                    # ê±°ë¦¬ ê³„ì‚° (Haversine formula)
                    distance = calculate_distance(center_lat, center_lng, place_lat, place_lng)
                    
                    # ë°˜ê²½ ë‚´ ì¥ì†Œë§Œ ì¶”ê°€
                    if distance <= radius:
                        places.append({
                            'name': place.get('name', ''),
                            'distance': int(distance),
                            'lat': place_lat,
                            'lng': place_lng
                        })
                except:
                    continue
        
        return places
        
    except Exception as e:
        print(f"        âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
        return []

def calculate_distance(lat1, lng1, lat2, lng2):
    """ë‘ ì¢Œí‘œ ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚° (ë¯¸í„°)"""
    from math import radians, sin, cos, sqrt, atan2
    
    R = 6371000  # ì§€êµ¬ ë°˜ì§€ë¦„ (ë¯¸í„°)
    
    lat1_rad = radians(lat1)
    lat2_rad = radians(lat2)
    delta_lat = radians(lat2 - lat1)
    delta_lng = radians(lng2 - lng1)
    
    a = sin(delta_lat/2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(delta_lng/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1-a))
    
    return R * c

def collect_pois_naver(address):
    """ë„¤ì´ë²„ë¡œ POI ìˆ˜ì§‘"""
    print(f"\n    ğŸ” ë„¤ì´ë²„ POI ìˆ˜ì§‘ ì‹œì‘")
    print(f"    ì£¼ì†Œ: {address}")
    
    coords = naver_geocode(address)
    if not coords:
        print(f"    âŒ ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨ - POI ìˆ˜ì§‘ ê±´ë„ˆëœ€")
        return None
    
    center_lat, center_lng = coords
    print(f"    âœ… ì¢Œí‘œ: ({center_lat:.6f}, {center_lng:.6f})")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ (ë„¤ì´ë²„ì— ìµœì í™”)
    categories = {
        "ì§€í•˜ì² ì—­": ["ì§€í•˜ì² ì—­", "ì „ì² ì—­"],
        "ì´ˆë“±í•™êµ": ["ì´ˆë“±í•™êµ"],
        "ì¤‘í•™êµ": ["ì¤‘í•™êµ"],
        "ëŒ€í˜•ë§ˆíŠ¸": ["ì´ë§ˆíŠ¸", "ë¡¯ë°ë§ˆíŠ¸", "í™ˆí”ŒëŸ¬ìŠ¤"],
        "ê´€ê´‘ì§€": ["í•´ìˆ˜ìš•ì¥", "ê³µì›"],
    }
    
    pois = {}
    total_found = 0
    
    for cat_name, keywords in categories.items():
        all_places = []
        for kw in keywords:
            places = naver_search_places(kw, center_lat, center_lng, radius=2000)
            all_places.extend(places)
            time.sleep(0.1)  # API í˜¸ì¶œ ê°„ê²©
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []
        for place in all_places:
            if place['name'] not in seen:
                seen.add(place['name'])
                unique.append(place)
        
        unique.sort(key=lambda x: x['distance'])
        if unique:
            pois[cat_name] = unique
            total_found += len(unique)
            print(f"    âœ… {cat_name}: {len(unique)}ê°œ ({unique[0]['name']} {unique[0]['distance']}m)")
        else:
            print(f"    âšª {cat_name}: 0ê°œ")
    
    if total_found > 0:
        print(f"    ğŸ“Š ì´ {total_found}ê°œ POI ìˆ˜ì§‘ ì™„ë£Œ")
        return pois
    else:
        print(f"    âš ï¸ POIë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        return None

# ====== 1. ëª©ë¡ ìˆ˜ì§‘ ======
def collect_posts(driver):
    urls = []
    seen_datano = set()  # dataNo ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ ì¶”ê°€
    
    for page in range(START_PAGE, END_PAGE + 1):
        url = f"{BASE_URL}&curPage={page}"
        print(f"\nâ–¶ í˜ì´ì§€ {page}/{END_PAGE}")
        
        driver.get(url)
        time.sleep(PAGE_SLEEP)
        
        try:
            WebDriverWait(driver, TIMEOUT).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr"))
            )
        except:
            continue
        
        rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
        print(f"  ğŸ“‹ ì „ì²´ í–‰ ìˆ˜: {len(rows)}")
        
        for idx, row in enumerate(rows, 1):
            try:
                links = row.find_elements(By.TAG_NAME, "a")
                title_link = None
                
                for link in links:
                    txt = safe_text(link)
                    if txt and txt not in {"ë¯¸ë¦¬ë³´ê¸°", "ë¯¸ë¦¬ë“£ê¸°"}:
                        title_link = link
                        break
                
                if not title_link:
                    continue
                
                title = safe_text(title_link)
                norm = normalize_text(title)
                
                # ë””ë²„ê¹…: ëª¨ë“  ê³µê³  ì¶œë ¥
                has_keyword = any(kw in norm for kw in KEYWORDS)
                status = "âœ…" if has_keyword else "âŠ˜"
                print(f"  {status} {title[:50]}")
                
                if has_keyword:
                    href = title_link.get_attribute("href")
                    
                    if href:
                        # dataNo ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
                        datano_match = re.search(r'dataNo=(\d+)', href)
                        if datano_match:
                            datano = datano_match.group(1)
                            
                            if datano not in seen_datano:
                                seen_datano.add(datano)
                                urls.append(href)
                            else:
                                print(f"     âŠ˜ ì¤‘ë³µ (dataNo: {datano})")
            except:
                continue
    
    print(f"\nğŸ“Œ ì´ {len(urls)}ê°œ")
    return urls

# ====== 2. ìƒì„¸ ì¶”ì¶œ ======
def extract_detail(driver, url):
    driver.get(url)
    time.sleep(0.5)
    WebDriverWait(driver, TIMEOUT).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    
    title = ""
    try:
        t = driver.find_element(By.XPATH, "//dl[contains(@class,'form-data-info')]//dt[normalize-space()='ì œëª©']/following-sibling::dd[1]//li[1]")
        title = safe_text(t)
    except:
        try:
            title = safe_text(driver.find_element(By.XPATH, "//h4[contains(@class,'form-data-subject')]"))
        except:
            pass
    
    attachments = []
    try:
        attach = driver.find_element(By.XPATH, "//dt[normalize-space()='ì²¨ë¶€íŒŒì¼']/following-sibling::dd[1]")
        anchors = attach.find_elements(By.TAG_NAME, "a")
        
        for a in anchors:
            txt = safe_text(a)
            if txt in {"ë¯¸ë¦¬ë³´ê¸°", "ë¯¸ë¦¬ë“£ê¸°"}:
                continue
            
            href = a.get_attribute("href")
            if href and "/comm/getFile" in href:
                attachments.append({
                    "filename": txt,
                    "url": href if href.startswith("http") else f"https://www.busan.go.kr{href}"
                })
    except:
        pass
    
    return {"url": url, "title": title, "attachments": attachments}

# ====== 3. ë‹¤ìš´ë¡œë“œ ======
def download_pdf(driver, files, referer, title):
    saved = []
    cookies = requests.cookies.RequestsCookieJar()
    for c in driver.get_cookies():
        cookies.set(c["name"], c["value"], domain=c.get("domain"), path=c.get("path", "/"))
    
    # SSL ìš°íšŒ ì„¤ì •
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    for idx, f in enumerate(files, 1):
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                headers = {
                    "Referer": referer,
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                
                resp = requests.get(
                    f["url"], 
                    cookies=cookies, 
                    headers=headers,
                    timeout=60,  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                    stream=True,
                    verify=False  # SSL ê²€ì¦ ìš°íšŒ
                )
                resp.raise_for_status()
                
                prefix = datetime.now().strftime("%Y%m%d")
                filename = f"{prefix}_{clean_filename(title)[:50]}_{idx}.pdf"
                path = os.path.join(OUT_DIR, filename)
                
                with open(path, "wb") as fp:
                    for chunk in resp.iter_content(8192):
                        fp.write(chunk)
                
                saved.append(path)
                print(f"    âœ… {Path(path).name}")
                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                
            except Exception as e:
                retry_count += 1
                if retry_count < max_retries:
                    print(f"    âš ï¸ ì¬ì‹œë„ {retry_count}/{max_retries}...")
                    time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                else:
                    print(f"    âœ— ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({max_retries}íšŒ ì‹œë„): {e}")
    
    return saved

# ====== 4. PDF â†’ ì´ë¯¸ì§€ ======
def pdf_to_images(pdf_path, title):
    """
    PyMuPDF(fitz)ë¡œ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (Poppler ë¶ˆí•„ìš”!)
    """
    try:
        import fitz  # PyMuPDF
    except:
        print("    âš ï¸ PyMuPDF ë¯¸ì„¤ì¹˜ (pip install PyMuPDF)")
        return []
    
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"    âš ï¸ PDF ì—´ê¸° ì‹¤íŒ¨: {e}")
        return []
    
    img_dir = Path(OUT_DIR) / "pdf_images" / clean_filename(title)
    img_dir.mkdir(parents=True, exist_ok=True)
    
    saved = []
    for page_num in range(len(doc)):
        try:
            page = doc.load_page(page_num)
            # 200 DPIë¡œ ë Œë”ë§ (matrixë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •)
            mat = fitz.Matrix(200/72, 200/72)  # 72 DPI â†’ 200 DPI
            pix = page.get_pixmap(matrix=mat)
            
            img_path = img_dir / f"page_{page_num + 1:03d}.png"
            pix.save(str(img_path))
            saved.append(str(img_path))
        except Exception as e:
            print(f"    âš ï¸ í˜ì´ì§€ {page_num + 1} ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    doc.close()
    print(f"    âœ… PDF ì´ë¯¸ì§€: {len(saved)}ì¥")
    return saved

# ====== 5. OCR ======
def ocr_pdf(pdf_path):
    """
    PyMuPDFë¡œ PDF ì´ë¯¸ì§€ ì¶”ì¶œ í›„ OCR
    """
    try:
        import fitz  # PyMuPDF
        import pytesseract
        from PIL import Image
        import io
    except:
        return "", {}
    
    try:
        pytesseract.pytesseract.tesseract_cmd = TESSERACT_EXE
        doc = fitz.open(pdf_path)
        
        parts = []
        max_pages = min(5, len(doc))  # ìµœëŒ€ 5í˜ì´ì§€ë§Œ OCR
        
        for page_num in range(max_pages):
            page = doc.load_page(page_num)
            # 150 DPIë¡œ ë Œë”ë§
            mat = fitz.Matrix(150/72, 150/72)
            pix = page.get_pixmap(matrix=mat)
            
            # PIL Imageë¡œ ë³€í™˜
            img_data = pix.tobytes("png")
            img = Image.open(io.BytesIO(img_data))
            
            # OCR ì‹¤í–‰
            txt = pytesseract.image_to_string(img, lang="kor+eng")
            parts.append(txt)
        
        doc.close()
        text = "\n".join(parts)
        return text, {"chars": len(text)}
    except Exception as e:
        print(f"    âœ— OCR ì‹¤íŒ¨: {e}")
        return "", {}

# ====== 6. ë¶„ì„ ======
def analyze_text(text, title):
    info = {"type": "ì¬ê±´ì¶•" if "ì¬ê±´ì¶•" in title else "ì¬ê°œë°œ"}
    
    # OCR í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¤„ë°”ê¿ˆ ì •ë¦¬
    text_clean = re.sub(r'\s+', ' ', text)  # ëª¨ë“  ê³µë°±ì„ ìŠ¤í˜ì´ìŠ¤ í•˜ë‚˜ë¡œ
    
    # ìœ„ì¹˜ - ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ
    found_addr = None
    
    # íŒ¨í„´ 1: "ìœ„ì¹˜" í‚¤ì›Œë“œ ì´í›„ ì°¾ê¸°
    loc_after_label = re.search(r'ìœ„\s*ì¹˜[:\s]*(.{5,80})', text_clean)
    if loc_after_label:
        after_text = loc_after_label.group(1)
        # ì´ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì†Œ ì¶”ì¶œ
        addr_patterns = [
            r'(ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*[ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
            r'([ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
        ]
        for p in addr_patterns:
            m = re.search(p, after_text)
            if m:
                found_addr = m.group(1).strip()
                print(f"    ğŸ“ ì£¼ì†Œ (ìœ„ì¹˜ í•„ë“œ): {found_addr}")
                break
    
    # íŒ¨í„´ 2: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ "êµ¬ + ë™ + ë²ˆì§€" ì§ì ‘ ì°¾ê¸°
    if not found_addr:
        direct_patterns = [
            r'(ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*[ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
            r'([ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
        ]
        for p in direct_patterns:
            m = re.search(p, text_clean)
            if m:
                found_addr = m.group(1).strip()
                print(f"    ğŸ“ ì£¼ì†Œ (ë³¸ë¬¸): {found_addr}")
                break
    
    # íŒ¨í„´ 3: ë™ + ë²ˆì§€ë§Œ ì°¾ì•„ì„œ ì¡°í•©
    if not found_addr:
        # êµ¬ ì°¾ê¸°
        gu_match = re.search(r'ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*([ê°€-í£]+êµ¬)', text_clean)
        gu = gu_match.group(1) if gu_match else None
        
        # ë™ + ë²ˆì§€ ì°¾ê¸°
        dong_benji = re.search(r'([ê°€-í£]+ë™)\s+(\d+(?:-\d+)?)\s*ë²ˆì§€', text_clean)
        if dong_benji:
            dong = dong_benji.group(1)
            benji = dong_benji.group(2)
            
            if gu:
                found_addr = f"ë¶€ì‚° {gu} {dong} {benji}ë²ˆì§€"
            else:
                found_addr = f"ë¶€ì‚° {dong} {benji}ë²ˆì§€"
            
            print(f"    ğŸ“ ì£¼ì†Œ (ì¡°í•©): {found_addr}")
    
    # íŒ¨í„´ 4: ì œëª©ì—ì„œ ë™ ì¶”ì¶œ + ë³¸ë¬¸ì—ì„œ êµ¬/ë²ˆì§€ ì°¾ê¸°
    if not found_addr:
        title_dong = re.search(r'([ê°€-í£]+ë™)', title)
        if title_dong:
            dong = title_dong.group(1)
            
            # ë³¸ë¬¸ì—ì„œ êµ¬ ì°¾ê¸°
            gu_match = re.search(r'ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*([ê°€-í£]+êµ¬)', text_clean)
            gu = gu_match.group(1) if gu_match else None
            
            # ë³¸ë¬¸ì—ì„œ ë²ˆì§€ ì°¾ê¸° (ë™ ì´ë¦„ ê·¼ì²˜)
            dong_idx = text_clean.find(dong)
            benji = None
            if dong_idx != -1:
                nearby = text_clean[max(0, dong_idx-50):dong_idx+100]
                benji_match = re.search(r'(\d+(?:-\d+)?)\s*ë²ˆì§€', nearby)
                if benji_match:
                    benji = benji_match.group(1)
            
            # ì¡°í•©
            if gu and benji:
                found_addr = f"ë¶€ì‚° {gu} {dong} {benji}ë²ˆì§€"
            elif gu:
                found_addr = f"ë¶€ì‚° {gu} {dong}"
            else:
                found_addr = f"ë¶€ì‚° {dong}"
            
            print(f"    ğŸ“ ì£¼ì†Œ (ì œëª©+ë³¸ë¬¸): {found_addr}")
    
    if found_addr:
        # ìµœì¢… ì •ë¦¬
        found_addr = re.sub(r'\s+', ' ', found_addr).strip()
        info["ìœ„ì¹˜"] = found_addr
    else:
        print(f"    âš ï¸ ì£¼ì†Œ ì¶”ì¶œ ì‹¤íŒ¨")
    
    # ë©´ì 
    area_m = re.search(r"(?:êµ¬ì—­)?ë©´ì [:\s]*([0-9,]+\.?\d*)\s*ã¡", text)
    if area_m:
        info["ë©´ì "] = f"{area_m.group(1)}ã¡"
    
    # ì„¸ëŒ€ìˆ˜
    house_m = re.search(r"(?:ì´\s*)?ì„¸ëŒ€ìˆ˜[:\s]*([0-9,]+)", text)
    if house_m:
        info["ì„¸ëŒ€ìˆ˜"] = f"{house_m.group(1)}ì„¸ëŒ€"
    
    # ë™ìˆ˜
    dong_m = re.search(r"([0-9]+)\s*ê°œ?\s*ë™", text)
    if dong_m:
        info["ë™ìˆ˜"] = f"{dong_m.group(1)}ê°œë™"
    
    # ì¸µìˆ˜
    floor_m = re.search(r"ì§€í•˜\s*(\d+).*ì§€ìƒ\s*(\d+)", text)
    if floor_m:
        info["ì¸µìˆ˜"] = f"ì§€í•˜{floor_m.group(1)}~ì§€ìƒ{floor_m.group(2)}ì¸µ"
    
    return info

# ====== 7. ë„¤ì´ë²„ ì§€ë„ ======
def capture_naver_map(addr, title):
    print(f"    ğŸ—ºï¸ ë„¤ì´ë²„ ì§€ë„ ìº¡ì²˜...")
    
    driver = None
    try:
        driver = make_driver(headless=HEADLESS_MAP)
        
        encoded_addr = urllib.parse.quote(addr)
        map_url = f"https://map.naver.com/v5/search/{encoded_addr}"
        
        driver.get(map_url)
        time.sleep(4)
        
        map_dir = Path(OUT_DIR) / "maps" / clean_filename(title)
        map_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œë“œë§µ
        road_path = str(map_dir / "naver_road.png")
        driver.save_screenshot(road_path)
        print(f"        âœ… ë¡œë“œë§µ")
        
        # ìœ„ì„±
        sat_path = ""
        try:
            sat_btn = driver.find_element(By.CSS_SELECTOR, "button[title='ìœ„ì„±'], button.btn_satellite")
            sat_btn.click()
            time.sleep(2)
            
            sat_path = str(map_dir / "naver_sat.png")
            driver.save_screenshot(sat_path)
            print(f"        âœ… ìœ„ì„±")
        except:
            print(f"        âš ï¸ ìœ„ì„± ë²„íŠ¼ ëª» ì°¾ìŒ")
        
        return map_url, [road_path, sat_path] if sat_path else [road_path]
    
    except Exception as e:
        print(f"        âœ— ì‹¤íŒ¨: {e}")
        return "", []
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

# ====== 8. HTML ìƒì„± ======
def build_html_with_poi(title, url, info, pdf_images, map_url, map_images, pois):
    """HTML ìƒì„± (ë„¤ì´ë²„ POI í¬í•¨)"""
    
    html = []
    
    location = info.get("ìœ„ì¹˜", "ë¶€ì‚°")
    event_type = info.get("type", "ì¬ê°œë°œ")
    
    area_match = re.search(r'([ê°€-í£0-9]+êµ¬ì—­|[ê°€-í£]+ë™)', title)
    area_name = area_match.group(1) if area_match else location.split()[-1] if location else "ë¶€ì‚°"
    
    gu_match = re.search(r'([ê°€-í£]+êµ¬)', location)
    gu_name = gu_match.group(1) if gu_match else ""
    
    main_keyword = f"{area_name} {event_type}"
    seo_title = f'{main_keyword} ì •ë¹„êµ¬ì—­ ì§€ì • | {gu_name} {event_type} ì™„ë²½ì •ë¦¬'
    
    html.append(f'<h1 style="color: #1a1a1a; font-size: 24px; font-weight: bold; margin-bottom: 20px;">{seo_title}</h1>')
    
    # ì¸íŠ¸ë¡œ
    html.append('<div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px;">')
    html.append(f'<p style="font-size: 16px; line-height: 1.8; margin: 0;">')
    html.append(f'<strong>{location}</strong>ì— <strong>{main_keyword}</strong> ì •ë¹„êµ¬ì—­ ì§€ì •ì´ ê³ ì‹œë˜ì—ˆìŠµë‹ˆë‹¤.')
    html.append('</p></div>')
    
    # ëª©ì°¨
    html.append('<div style="background: #fff; border: 1px solid #e0e0e0; padding: 15px; border-radius: 8px; margin-bottom: 30px;">')
    html.append('<p style="font-weight: bold; margin-bottom: 10px;">ëª©ì°¨</p>')
    html.append('<p style="font-size: 14px; line-height: 2.0; margin: 0;">')
    html.append('1. ìœ„ì¹˜<br>2. ì‚¬ì—… ê·œëª¨<br>3. ì£¼ë³€ ì‹œì„¤ (2km ë°˜ê²½)<br>4. êµ¬ì—­ ê²½ê³„<br>5. ì§„í–‰ ì¼ì •')
    html.append('</p></div>')
    
    html.append('<hr style="border-top: 2px solid #e0e0e0; margin: 30px 0;">')
    
    # 1. ìœ„ì¹˜
    if info.get("ìœ„ì¹˜"):
        html.append(f'<h2 style="color: #2c3e50; font-size: 20px; margin-top: 40px; font-weight: bold;">1. ì–´ë””ì— ì§€ì–´ì§€ë‚˜ìš”?</h2>')
        html.append(f'<p style="font-size: 15px; line-height: 1.8;">')
        html.append(f'<strong>{main_keyword}</strong>ì€ <strong>{info["ìœ„ì¹˜"]}</strong>ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.')
        html.append('</p>')
        
        if map_url:
            html.append(f'<p style="margin-top: 15px;"><a href="{map_url}" target="_blank" style="display: inline-block; padding: 10px 20px; background: #0066cc; color: white; text-decoration: none; border-radius: 5px;">ë„¤ì´ë²„ ì§€ë„ë¡œ ë³´ê¸°</a></p>')
        
        html.append('<hr style="border-top: 1px solid #e0e0e0; margin: 30px 0;">')
    
    # 2. ì‚¬ì—… ê·œëª¨
    html.append(f'<h2 style="color: #2c3e50; font-size: 20px; margin-top: 40px; font-weight: bold;">2. ì‚¬ì—… ê·œëª¨ëŠ”?</h2>')
    html.append('<div style="background: #f8f9fa; padding: 20px; border-radius: 8px;">')
    html.append('<ul style="line-height: 2.2; font-size: 15px; margin: 0;">')
    
    if info.get("ë©´ì "):
        html.append(f'<li><strong>ë©´ì :</strong> {info["ë©´ì "]}</li>')
    if info.get("ì„¸ëŒ€ìˆ˜"):
        html.append(f'<li><strong>ì„¸ëŒ€ìˆ˜:</strong> {info["ì„¸ëŒ€ìˆ˜"]}</li>')
    if info.get("ë™ìˆ˜"):
        html.append(f'<li><strong>ë™ìˆ˜:</strong> {info["ë™ìˆ˜"]}</li>')
    if info.get("ì¸µìˆ˜"):
        html.append(f'<li><strong>ì¸µìˆ˜:</strong> {info["ì¸µìˆ˜"]}</li>')
    
    html.append('</ul></div>')
    html.append('<hr style="border-top: 1px solid #e0e0e0; margin: 30px 0;">')
    
    # 3. ì£¼ë³€ ì‹œì„¤ (ë„¤ì´ë²„ POI)
    if pois:
        html.append(f'<h2 style="color: #2c3e50; font-size: 20px; margin-top: 40px; font-weight: bold;">3. ì£¼ë³€ ì‹œì„¤ (2km ë°˜ê²½)</h2>')
        
        # ê¹”ë”í•œ í‘œ
        html.append("""
<style>
.poi-table {
    border-collapse: collapse;
    width: auto;
    margin: 20px auto;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}
.poi-table th {
    background: #03C75A;
    color: white;
    padding: 12px 20px;
    text-align: center;
    font-weight: bold;
    border: 1px solid #ddd;
}
.poi-table td {
    border: 1px solid #ddd;
    padding: 10px 20px;
}
.poi-table td:first-child {
    text-align: center;
    font-weight: bold;
    color: #2c3e50;
}
.poi-table td:nth-child(2) {
    text-align: left;
}
.poi-table td:last-child {
    text-align: center;
    font-weight: bold;
    color: #e74c3c;
}
.poi-table tr:nth-child(even) {
    background: #f8f9fa;
}
</style>
""")
        
        html.append('<table class="poi-table">')
        html.append('<tr><th>ì¹´í…Œê³ ë¦¬</th><th>ì´ë¦„</th><th>ê±°ë¦¬(m)</th></tr>')
        
        for cat, places in pois.items():
            if places:
                place = places[0]
                html.append('<tr>')
                html.append(f'<td>{cat}</td>')
                html.append(f'<td>{place["name"]}</td>')
                html.append(f'<td>{place["distance"]}</td>')
                html.append('</tr>')
        
        html.append('</table>')
        html.append('<p style="text-align: center; color: #7f8c8d; font-size: 0.9em;">â€» ë„¤ì´ë²„ ì§€ë„ ê¸°ì¤€</p>')
        
        html.append('<hr style="border-top: 1px solid #e0e0e0; margin: 30px 0;">')
    
    # 4. êµ¬ì—­ ê²½ê³„
    if pdf_images:
        html.append(f'<h2 style="color: #2c3e50; font-size: 20px; margin-top: 40px; font-weight: bold;">4. êµ¬ì—­ ê²½ê³„ëŠ”?</h2>')
        html.append('<p style="color: #666; font-size: 14px;">â€» PDF ì´ë¯¸ì§€ë¥¼ ë¸”ë¡œê·¸ì— ì—…ë¡œë“œí•˜ì„¸ìš”</p>')
        html.append(f'<p style="color: #999; font-size: 13px;">ìœ„ì¹˜: pdf_images í´ë” (ì´ {len(pdf_images)}ì¥)</p>')
        html.append('<hr style="border-top: 1px solid #e0e0e0; margin: 30px 0;">')
    
    # 5. ì§„í–‰ ì¼ì •
    html.append(f'<h2 style="color: #2c3e50; font-size: 20px; margin-top: 40px; font-weight: bold;">5. ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì§„í–‰ë˜ë‚˜ìš”?</h2>')
    html.append('<ol style="line-height: 2.2; font-size: 15px;">')
    html.append('<li><strong>í˜„ì¬:</strong> ì •ë¹„êµ¬ì—­ ì§€ì • ê³ ì‹œ</li>')
    html.append('<li><strong>ë‹¤ìŒ:</strong> ì¶”ì§„ìœ„ì›íšŒ êµ¬ì„± â†’ ì¡°í•© ì„¤ë¦½</li>')
    html.append('<li><strong>ì˜ˆìƒ:</strong> ì¡°í•©ì„¤ë¦½ í›„ ì•½ 5~7ë…„</li>')
    html.append('</ol>')
    
    html.append('<div style="background: #e8f5e9; padding: 15px; border-radius: 8px; margin-top: 15px;">')
    html.append(f'<p style="font-size: 14px; margin: 0;">â€» í˜„ì¬ëŠ” ì •ë¹„êµ¬ì—­ ì§€ì • ë‹¨ê³„ì…ë‹ˆë‹¤. ì‹¤ì œ ì…ì£¼ê¹Œì§€ëŠ” ìµœì†Œ 5ë…„ ì´ìƒ ì†Œìš”ë©ë‹ˆë‹¤.</p>')
    html.append('</div>')
    
    html.append('<hr style="border-top: 2px solid #e0e0e0; margin: 40px 0;">')
    
    # ì›ë¬¸
    html.append('<h2 style="color: #2c3e50; font-size: 20px; margin-top: 40px;">ì›ë¬¸ ë³´ê¸°</h2>')
    html.append(f'<p><a href="{url}" target="_blank" style="color: #0066cc; text-decoration: underline; font-weight: bold;">ë¶€ì‚°ì‹œ ê³ ì‹œê³µê³  ì›ë¬¸</a></p>')
    
    html.append('<div style="background: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 20px;">')
    html.append('<p style="color: #666; font-size: 13px; margin: 0;">')
    html.append(f'â€» {datetime.now().strftime("%Yë…„ %mì›” %dì¼")} ê¸°ì¤€<br>')
    html.append('â€» íˆ¬ì íŒë‹¨ì€ ì „ë¬¸ê°€ ìƒë‹´ í›„ ê²°ì •í•˜ì„¸ìš”')
    html.append('</p></div>')
    
    return '\n'.join(html)

# ====== ë©”ì¸ ì²˜ë¦¬ ======
def run_once(driver, detail_url, writer):
    print(f"\n{'='*80}")
    print(f"ì²˜ë¦¬: {detail_url}")
    
    data = extract_detail(driver, detail_url)
    title = data["title"]
    files = data["attachments"]
    
    print(f"\nì œëª©: {title}")
    
    if not files:
        print("âŒ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")
        if writer:
            writer.writerow([detail_url, title, "(no-file)"])
        return
    
    # ë‹¤ìš´ë¡œë“œ
    pdfs = download_pdf(driver, files, detail_url, title)
    if not pdfs:
        if writer:
            writer.writerow([detail_url, title, "(download-fail)"])
        return
    
    pdf_path = pdfs[0]
    
    # PDF â†’ ì´ë¯¸ì§€
    print("\n  ğŸ“„ PDF â†’ ì´ë¯¸ì§€")
    pdf_images = pdf_to_images(pdf_path, title)
    
    # OCR
    print("\n  ğŸ” OCR")
    text, meta = ocr_pdf(pdf_path)
    
    if meta.get("chars", 0) < OCR_MIN_CHARS:
        print(f"    âŒ OCR í’ˆì§ˆ ë¯¸ë‹¬")
        if writer:
            writer.writerow([detail_url, title, "(ocr-low)"])
        return
    
    print(f"    âœ… {meta.get('chars')}ì")
    
    # í…ìŠ¤íŠ¸ ì €ì¥
    txt_dir = Path(OUT_DIR) / "txt"
    prefix = datetime.now().strftime("%Y%m%d")
    txt_name = f"{prefix}_{clean_filename(title)}.txt"
    with open(txt_dir / txt_name, "w", encoding="utf-8") as f:
        f.write(text)
    
    # ë¶„ì„
    print("\n  ğŸ“Š ë¶„ì„")
    info = analyze_text(text, title)
    print(f"    ìœ í˜•: {info.get('type')}")
    print(f"    ìœ„ì¹˜: {info.get('ìœ„ì¹˜', '(ë¯¸ì¶”ì¶œ)')}")
    
    # ì§€ë„
    map_url = ""
    map_images = []
    addr = info.get("ìœ„ì¹˜", "")
    if addr:
        map_url, map_images = capture_naver_map(addr, title)
    
    # ë„¤ì´ë²„ POI
    print("\n  ğŸª ì£¼ë³€ ì‹œì„¤ ì¡°ì‚¬ (ë„¤ì´ë²„ API)")
    pois = None
    if addr:
        print(f"    ëŒ€ìƒ ì£¼ì†Œ: {addr}")
        pois = collect_pois_naver(addr)
        
        if pois:
            print(f"    âœ… POI ìˆ˜ì§‘ ì„±ê³µ: {len(pois)}ê°œ ì¹´í…Œê³ ë¦¬")
        else:
            print(f"    âš ï¸ POI ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ")
    else:
        print(f"    âš ï¸ ì£¼ì†Œ ì¶”ì¶œ ì‹¤íŒ¨ - POI ìˆ˜ì§‘ ë¶ˆê°€")
    
    # HTML
    html_dir = Path(OUT_DIR) / "blog_html"
    html = build_html_with_poi(title, detail_url, info, pdf_images, map_url, map_images, pois)
    
    html_name = f"{prefix}_{clean_filename(title)}.html"
    with open(html_dir / html_name, "w", encoding="utf-8") as f:
        f.write(html)
    
    print(f"\n  âœ… HTML: {html_name}")
    
    # í´ë¦½ë³´ë“œ
    if pyperclip:
        try:
            pyperclip.copy(html)
            print("  âœ… í´ë¦½ë³´ë“œ ë³µì‚¬!")
        except:
            pass
    
    if writer:
        writer.writerow([detail_url, title, html_name])
    
    print(f"{'='*80}\n")

def main():
    ensure_dirs()
    
    print("\n" + "="*80)
    print("ë¶€ì‚° ê³ ì‹œê³µê³  â†’ ë¸”ë¡œê·¸ ìë™í™” (ë„¤ì´ë²„ API ë²„ì „)")
    print("="*80)
    
    driver = make_driver(headless=HEADLESS_LIST)
    
    try:
        urls = collect_posts(driver)
        
        if not urls:
            print("\nâŒ ë§¤ì¹­ ì—†ìŒ")
            return
        
        csv_path = Path(CSV_PATH)
        try:
            fp = open(csv_path, "w", newline="", encoding="utf-8-sig")
        except:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_path = csv_path.with_name(f"{csv_path.stem}_{ts}{csv_path.suffix}")
            fp = open(csv_path, "w", newline="", encoding="utf-8-sig")
        
        with fp:
            writer = csv.writer(fp)
            writer.writerow(["url", "title", "html"])
            
            for idx, url in enumerate(urls, 1):
                print(f"\n[{idx}/{len(urls)}]")
                run_once(driver, url, writer)
                time.sleep(1)
        
        print("\n" + "="*80)
        print("âœ… ì™„ë£Œ!")
        print(f"\nğŸ“‚ ì¶œë ¥:")
        print(f"   HTML: {Path(OUT_DIR) / 'blog_html'}")
        print(f"   PDF ì´ë¯¸ì§€: {Path(OUT_DIR) / 'pdf_images'}")
        print(f"   ì§€ë„: {Path(OUT_DIR) / 'maps'}")
        print("="*80)
    
    finally:
        driver.quit()

if __name__ == "__main__":
    main()
