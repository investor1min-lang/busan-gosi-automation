# -*- coding: utf-8 -*-
"""
ë¶€ì‚° ê³ ì‹œê³µê³  í¬ë¡¤ëŸ¬ (GitHub Actions ìµœì í™” ë²„ì „)
"""

import os
import re
import time
import requests
from pathlib import Path
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ====== CONFIG ======
BASE_URL = "https://www.busan.go.kr/news/gosiboard?articlNo=2"
START_PAGE = 1
END_PAGE = 1
KEYWORDS = ["ì¬ê°œë°œ", "ì¬ê±´ì¶•"]

# GitHub Actions í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
OUT_DIR = os.path.join(os.getcwd(), "downloaded_files")
HEADLESS_LIST = True
PAGE_SLEEP = 0.8
TIMEOUT = 15

# TesseractëŠ” GitHub Actionsì—ì„œ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ë¯€ë¡œ ê²½ë¡œ ë¶ˆí•„ìš”
# PyMuPDFëŠ” pip installë¡œ ì„¤ì¹˜
OCR_MIN_CHARS = 300

# ====== ìœ í‹¸ í•¨ìˆ˜ ======
def ensure_dirs():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    for p in [OUT_DIR, 
              os.path.join(OUT_DIR, "txt"), 
              os.path.join(OUT_DIR, "blog_html"),
              os.path.join(OUT_DIR, "pdf_images"), 
              os.path.join(OUT_DIR, "maps"),
              os.path.join(OUT_DIR, "gosi_html"),
              os.path.join(OUT_DIR, "screenshots")]:
        Path(p).mkdir(parents=True, exist_ok=True)

def make_driver(headless=True):
    """Chrome WebDriver ìƒì„±"""
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1400,1100")
    return webdriver.Chrome(options=opts)

def safe_text(el):
    """ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        return el.text.strip()
    except:
        return ""

def clean_filename(name):
    """íŒŒì¼ëª… ì •ë¦¬"""
    name = re.sub(r"\s*\(ìš©ëŸ‰[^)]*\)\s*$", "", name or "").strip()
    name = re.sub(r'[\\/:*?"<>|]', "_", name)
    return (name or "unnamed")[:180]

def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    t = (text or "").lower()
    t = re.sub(r"\s+", " ", t)
    return t.strip()

# ====== 1. ëª©ë¡ ìˆ˜ì§‘ ======
def collect_posts(driver):
    """ê³ ì‹œê³µê³  ëª©ë¡ ìˆ˜ì§‘"""
    urls = []
    seen_datano = set()
    
    for page in range(START_PAGE, END_PAGE + 1):
        url = f"{BASE_URL}&curPage={page}"
        print(f"\nâ–¶ í˜ì´ì§€ {page}/{END_PAGE}")
        
        driver.get(url)
        time.sleep(PAGE_SLEEP)
        
        try:
            WebDriverWait(driver, TIMEOUT).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr"))
            )
        except:
            continue
        
        rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
        
        for row in rows:
            try:
                links = row.find_elements(By.TAG_NAME, "a")
                title_link = None
                
                for link in links:
                    txt = safe_text(link)
                    if txt and txt not in {"ë¯¸ë¦¬ë³´ê¸°", "ë¯¸ë¦¬ë“£ê¸°"}:
                        title_link = link
                        break
                
                if not title_link:
                    continue
                
                title = safe_text(title_link)
                norm = normalize_text(title)
                
                if any(kw in norm for kw in KEYWORDS):
                    href = title_link.get_attribute("href")
                    
                    if href:
                        # dataNo ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
                        datano_match = re.search(r'dataNo=(\d+)', href)
                        if datano_match:
                            datano = datano_match.group(1)
                            
                            if datano not in seen_datano:
                                seen_datano.add(datano)
                                urls.append(href)
                                print(f"  âœ… {title[:60]}")
            except:
                continue
    
    print(f"\nğŸ“Œ ì´ {len(urls)}ê°œ")
    return urls

# ====== 2. ìƒì„¸ ì¶”ì¶œ ======
def extract_detail(driver, url):
    """ê³µê³  ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
    driver.get(url)
    time.sleep(0.5)
    WebDriverWait(driver, TIMEOUT).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    
    title = ""
    try:
        t = driver.find_element(By.XPATH, "//dl[contains(@class,'form-data-info')]//dt[normalize-space()='ì œëª©']/following-sibling::dd[1]//li[1]")
        title = safe_text(t)
    except:
        try:
            title = safe_text(driver.find_element(By.XPATH, "//h4[contains(@class,'form-data-subject')]"))
        except:
            pass
    
    attachments = []
    try:
        attach = driver.find_element(By.XPATH, "//dt[normalize-space()='ì²¨ë¶€íŒŒì¼']/following-sibling::dd[1]")
        anchors = attach.find_elements(By.TAG_NAME, "a")
        
        for a in anchors:
            txt = safe_text(a)
            if txt in {"ë¯¸ë¦¬ë³´ê¸°", "ë¯¸ë¦¬ë“£ê¸°"}:
                continue
            
            href = a.get_attribute("href")
            if href and "/comm/getFile" in href:
                attachments.append({
                    "filename": txt,
                    "url": href if href.startswith("http") else f"https://www.busan.go.kr{href}"
                })
    except:
        pass
    
    return {"url": url, "title": title, "attachments": attachments}

# ====== 3. PDF ë‹¤ìš´ë¡œë“œ ======
def download_pdf(driver, files, referer, title):
    """PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    saved = []
    cookies = requests.cookies.RequestsCookieJar()
    for c in driver.get_cookies():
        cookies.set(c["name"], c["value"], domain=c.get("domain"), path=c.get("path", "/"))
    
    for idx, f in enumerate(files, 1):
        try:
            resp = requests.get(f["url"], cookies=cookies, headers={"Referer": referer}, timeout=30, stream=True)
            resp.raise_for_status()
            
            prefix = datetime.now().strftime("%Y%m%d")
            filename = f"{prefix}_{clean_filename(title)[:50]}_{idx}.pdf"
            path = os.path.join(OUT_DIR, filename)
            
            with open(path, "wb") as fp:
                for chunk in resp.iter_content(8192):
                    fp.write(chunk)
            
            saved.append(path)
            print(f"    âœ… {Path(path).name}")
        except Exception as e:
            print(f"    âœ— ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return saved

# ====== 4. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ======
def pdf_to_images(pdf_path, title):
    """
    PyMuPDF(fitz)ë¡œ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    """
    try:
        import fitz  # PyMuPDF
    except:
        print("    âš ï¸ PyMuPDF ë¯¸ì„¤ì¹˜ (pip install PyMuPDF)")
        return []
    
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"    âš ï¸ PDF ì—´ê¸° ì‹¤íŒ¨: {e}")
        return []
    
    img_dir = Path(OUT_DIR) / "pdf_images" / clean_filename(title)
    img_dir.mkdir(parents=True, exist_ok=True)
    
    saved = []
    for page_num in range(len(doc)):
        try:
            page = doc.load_page(page_num)
            # 200 DPIë¡œ ë Œë”ë§
            mat = fitz.Matrix(200/72, 200/72)
            pix = page.get_pixmap(matrix=mat)
            
            img_path = img_dir / f"page_{page_num + 1:03d}.png"
            pix.save(str(img_path))
            saved.append(str(img_path))
        except Exception as e:
            print(f"    âš ï¸ í˜ì´ì§€ {page_num + 1} ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    doc.close()
    print(f"    âœ… PDF ì´ë¯¸ì§€: {len(saved)}ì¥")
    return saved

# ====== 5. OCR ======
def ocr_pdf(pdf_path):
    """
    PyMuPDFë¡œ PDF ì´ë¯¸ì§€ ì¶”ì¶œ í›„ Tesseract OCR
    """
    try:
        import fitz  # PyMuPDF
        import pytesseract
        from PIL import Image
        import io
    except ImportError as e:
        print(f"    âš ï¸ ëª¨ë“ˆ ëˆ„ë½: {e}")
        return "", {}
    
    try:
        # GitHub Actionsì—ì„œëŠ” tesseractê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆìŒ
        # ê²½ë¡œ ì§€ì • ë¶ˆí•„ìš” (ìë™ íƒì§€)
        doc = fitz.open(pdf_path)
        
        parts = []
        max_pages = min(5, len(doc))  # ìµœëŒ€ 5í˜ì´ì§€ë§Œ OCR
        
        for page_num in range(max_pages):
            page = doc.load_page(page_num)
            # 150 DPIë¡œ ë Œë”ë§
            mat = fitz.Matrix(150/72, 150/72)
            pix = page.get_pixmap(matrix=mat)
            
            # PIL Imageë¡œ ë³€í™˜
            img_data = pix.tobytes("png")
            img = Image.open(io.BytesIO(img_data))
            
            # OCR ì‹¤í–‰
            txt = pytesseract.image_to_string(img, lang="kor+eng")
            parts.append(txt)
        
        doc.close()
        text = "\n".join(parts)
        return text, {"chars": len(text)}
    except Exception as e:
        print(f"    âœ— OCR ì‹¤íŒ¨: {e}")
        return "", {}

# ====== 6. í…ìŠ¤íŠ¸ ë¶„ì„ ======
def analyze_text(text, title):
    """í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    info = {"type": "ì¬ê±´ì¶•" if "ì¬ê±´ì¶•" in title else "ì¬ê°œë°œ"}
    
    # OCR í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    text_clean = re.sub(r'\s+', ' ', text)
    
    # ìœ„ì¹˜ ì¶”ì¶œ - ë‹¤ì–‘í•œ íŒ¨í„´
    found_addr = None
    
    # íŒ¨í„´ 1: "ìœ„ì¹˜" í‚¤ì›Œë“œ ì´í›„
    loc_after_label = re.search(r'ìœ„\s*ì¹˜[:\s]*(.{5,80})', text_clean)
    if loc_after_label:
        after_text = loc_after_label.group(1)
        addr_patterns = [
            r'(ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*[ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
            r'([ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
        ]
        for p in addr_patterns:
            m = re.search(p, after_text)
            if m:
                found_addr = m.group(1).strip()
                print(f"    ğŸ“ ì£¼ì†Œ (ìœ„ì¹˜ í•„ë“œ): {found_addr}")
                break
    
    # íŒ¨í„´ 2: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì°¾ê¸°
    if not found_addr:
        direct_patterns = [
            r'(ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*[ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
            r'([ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+(?:-\d+)?(?:\s*ë²ˆì§€)?(?:\s*ì¼ì›)?)',
        ]
        for p in direct_patterns:
            m = re.search(p, text_clean)
            if m:
                found_addr = m.group(1).strip()
                print(f"    ğŸ“ ì£¼ì†Œ (ë³¸ë¬¸): {found_addr}")
                break
    
    # íŒ¨í„´ 3: ì œëª©ì—ì„œ ë™ ì¶”ì¶œ + ë³¸ë¬¸ì—ì„œ êµ¬/ë²ˆì§€ ì¡°í•©
    if not found_addr:
        title_dong = re.search(r'([ê°€-í£]+ë™)', title)
        if title_dong:
            dong = title_dong.group(1)
            
            # ë³¸ë¬¸ì—ì„œ êµ¬ ì°¾ê¸°
            gu_match = re.search(r'ë¶€ì‚°(?:ê´‘ì—­ì‹œ)?\s*([ê°€-í£]+êµ¬)', text_clean)
            gu = gu_match.group(1) if gu_match else None
            
            # ë³¸ë¬¸ì—ì„œ ë²ˆì§€ ì°¾ê¸°
            dong_idx = text_clean.find(dong)
            benji = None
            if dong_idx != -1:
                nearby = text_clean[max(0, dong_idx-50):dong_idx+100]
                benji_match = re.search(r'(\d+(?:-\d+)?)\s*ë²ˆì§€', nearby)
                if benji_match:
                    benji = benji_match.group(1)
            
            # ì¡°í•©
            if gu and benji:
                found_addr = f"ë¶€ì‚° {gu} {dong} {benji}ë²ˆì§€"
            elif gu:
                found_addr = f"ë¶€ì‚° {gu} {dong}"
            else:
                found_addr = f"ë¶€ì‚° {dong}"
            
            print(f"    ğŸ“ ì£¼ì†Œ (ì œëª©+ë³¸ë¬¸): {found_addr}")
    
    if found_addr:
        found_addr = re.sub(r'\s+', ' ', found_addr).strip()
        info["ìœ„ì¹˜"] = found_addr
    else:
        print(f"    âš ï¸ ì£¼ì†Œ ì¶”ì¶œ ì‹¤íŒ¨")
    
    # ë©´ì 
    area_m = re.search(r"(?:êµ¬ì—­)?ë©´ì [:\s]*([0-9,]+\.?\d*)\s*ã¡", text)
    if area_m:
        info["ë©´ì "] = f"{area_m.group(1)}ã¡"
    
    # ì„¸ëŒ€ìˆ˜
    house_m = re.search(r"(?:ì´\s*)?ì„¸ëŒ€ìˆ˜[:\s]*([0-9,]+)", text)
    if house_m:
        info["ì„¸ëŒ€ìˆ˜"] = f"{house_m.group(1)}ì„¸ëŒ€"
    
    # ë™ìˆ˜
    dong_m = re.search(r"([0-9]+)\s*ê°œ?\s*ë™", text)
    if dong_m:
        info["ë™ìˆ˜"] = f"{dong_m.group(1)}ê°œë™"
    
    # ì¸µìˆ˜
    floor_m = re.search(r"ì§€í•˜\s*(\d+).*ì§€ìƒ\s*(\d+)", text)
    if floor_m:
        info["ì¸µìˆ˜"] = f"ì§€í•˜{floor_m.group(1)}~ì§€ìƒ{floor_m.group(2)}ì¸µ"
    
    return info

# ====== í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ======
if __name__ == "__main__":
    print("="*80)
    print("ë¶€ì‚° ê³ ì‹œê³µê³  í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    ensure_dirs()
    
    driver = make_driver(headless=HEADLESS_LIST)
    
    try:
        urls = collect_posts(driver)
        
        if urls:
            print(f"\nì²« ë²ˆì§¸ ê³µê³  í…ŒìŠ¤íŠ¸:")
            detail = extract_detail(driver, urls[0])
            print(f"ì œëª©: {detail['title']}")
            print(f"ì²¨ë¶€: {len(detail['attachments'])}ê°œ")
    finally:
        driver.quit()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
